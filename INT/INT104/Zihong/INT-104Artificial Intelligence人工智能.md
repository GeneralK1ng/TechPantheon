# INT-104 Artificial Intelligence äººå·¥æ™ºèƒ½


![](./Untitled.png)

# Week 1 **ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½**

### **ä»€ä¹ˆæ˜¯AI?**

1. AI is the study of complex information processing problems that often have their roots in some aspect of biological information processing. The goal of the subject is to identify solvable and interesting information processing problems, and solve them. (David Marr)
2. The intelligent connection of perception to action (Rodney Brooks)
3. Actions that are indistinguishable from a humanâ€™s (Alan Turing)

### **å½“ä¸‹AI**

Â· å¹¿æ³›æˆä¸ºå­¦ä¹ ç”Ÿæ´»ä¸­çš„å·¥å…·-ï¼ˆç¥ç»ç½‘ç»œï¼Œéšè—é©¬å°”å¯å¤«æ¨¡å‹ï¼Œè´å¶æ–¯ç½‘ç»œï¼Œå¯å‘å¼æœç´¢ï¼‰

Â· ä»¥æ¨¡å‹ï¼Œæ¦‚ç‡ï¼Œç»Ÿè®¡ï¼Œä¼˜åŒ–ï¼Œç®—æ³•ä¸ºåŸºç¡€çš„äº¤å‰å­¦ç§‘

### **äººå·¥æ™ºèƒ½çš„å›°éš¾**

1. å¤§æ•°æ®é›† 
2. æœ€ä¼˜åŒ– 
3. å™ªç‚¹ä¸ç¼ºå¤±æ•°æ® 
4. NP hard 
5. æè¿°é—®é¢˜ï¼ˆèŒƒåŒ–Paradigmï¼‰
6. å¯»æ‰¾å¥å£®çš„ç®—æ³•

# Week 2 DATA PREPROCESSING

éš”ä½ç½®åˆ‡ç‰‡

```python
éš”ä½ç½®åˆ‡ç‰‡æ˜¯æ•°ä¹‹åçš„ç¬¬å‡ ä¸ªç„¶åå–ï¼Œå¦‚ä¾‹å­æ˜¯æ¯ä¸¤ä¸ªå–ç¬¬äºŒä¸ªï¼Œè€Œä¸æ˜¯éš”2ä¸ªå–ä¸€ä¸ªã€‚
testlist=[0,1,2,3,4,5,6,7]
print(testlist[0:-1:2])

#------out------[0, 2, 4, 6]

```

zip()

```python
zip()å°†å¯è¿­ä»£å¯¹è±¡ç»„æˆå…ƒç»„Tuple
key=[0,1,2]
val1=['a','b','c']
val2=['A','B','C']
print(list(zip(key,val1,val2)))
#------out------[(0, 'a', 'A'), (1, 'b', 'B'), (2, 'c', 'C')]

```

åˆ—è¡¨è§£æå¼

```python
testtuple=[(0, 'a', 'A'), (1, 'b', 'B'), (2, 'c', 'C')]
print([ch1+ch2 for (num,ch1,ch2) in testtuple])
#------out------['aA', 'bB', 'cC']

```

è¿­ä»£å™¨

```python
def f(n):
    # ç”Ÿæˆå™¨å‡½æ•°ï¼Œä¾æ¬¡äº§ç”Ÿ n, n+1, n+2
    yield n
    yield n + 1
    yield n + 2

# åˆ›å»ºç”Ÿæˆå™¨å¯¹è±¡
item = f(5)

# æ‰“å°ç¬¬ä¸€æ¬¡è°ƒç”¨nextæ—¶çš„ç»“æœ
print(next(item))  # 5

# å†æ¬¡è°ƒç”¨nextå¹¶æ‰“å°ç»“æœ
print(next(item))  # 6

# ç¬¬ä¸‰æ¬¡è°ƒç”¨nextå¹¶æ‰“å°ç»“æœ
print(next(item))  # 7

# ç”±äºç”Ÿæˆå™¨å·²ç»è€—å°½ï¼Œæ²¡æœ‰æ›´å¤šçš„å€¼ï¼Œsumä¼šè®¡ç®—ç©ºç”Ÿæˆå™¨çš„å’Œï¼Œç»“æœä¸º0
print("sum after next 3 times:", sum(item))  # 0

# åˆ›å»ºæ–°çš„ç”Ÿæˆå™¨å¯¹è±¡
newitem = f(5)

# è®¡ç®—æ–°ç”Ÿæˆå™¨å¯¹è±¡ä¸­æ‰€æœ‰å…ƒç´ çš„å’Œ
print("newitem sum:", sum(newitem))  # 18

```

## 2.1 Data Type

- Structured               Example: tables
    - Highly organized
    - Usually with a label
- Unstructured           Example: free text

## 2.2 Data Collection

![](./Untitled1.png)

## 2.3 Data Storage and Presentation

### 2.3.1 CSV (Comma Separated Values)

![](./Untitled2.png)

### 2.3.2 TSV (Tab Separated Values)

![](./Untitled3.png)

### 2.3.3 XML (Extensible Markup Language)

- æ˜¯ä¸€ç§å¯æ‰©å±•æ ‡è®°è¯­è¨€ï¼Œç”¨äºå®šä¹‰æ–‡æ¡£çš„ç»“æ„å’Œå†…å®¹ã€‚
- XMLçš„åŸºæœ¬ç»“æ„æ˜¯ç”±ä¸€ç³»åˆ—æ ‡ç­¾ï¼ˆå³å¼€å§‹æ ‡ç­¾å’Œç»“æŸæ ‡ç­¾ï¼‰ç»„æˆçš„ã€‚
    - è¿™äº›æ ‡ç­¾ç”¨äºæ ‡è¯†æ•°æ®çš„ç»“æ„å’Œå«ä¹‰ã€‚
    - ä¸HTMLä¸åŒï¼ŒXMLæ²¡æœ‰é¢„å®šä¹‰çš„æ ‡ç­¾ï¼Œè€Œæ˜¯å…è®¸ç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚å®šä¹‰æ ‡ç­¾ã€‚è¿™ä½¿å¾—XMLéå¸¸çµæ´»ï¼Œé€‚ç”¨äºå„ç§ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚

![](./Untitled4.png)

### 2.3.4 JSON (JavaScript Object Notation)

![](./Untitled5.png)

## 2.4 Data Visualization

![](./Untitled6.png)

## 2.5 Data Pre-processing

- Data cleaning

![](./Untitled7.png)

- Data Integration æ•°æ®æ•´åˆ

![](./Untitled8.png)

- Data transformation

![](./Untitled9.png)

- Data reduction

Data reduction is a key process in which a reduced representation of a dataset that produces the same or similar analytical results is obtained.

## 2.6 Feature Selection

- Filter methods â€“ features are selected and ranked according to their relationships with the target
    - åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œç‰¹å¾æ ¹æ®å®ƒä»¬ä¸ç›®æ ‡å˜é‡ä¹‹é—´çš„å…³ç³»è¿›è¡Œé€‰æ‹©å’Œæ’åºï¼Œè€Œä¸è€ƒè™‘ä»»ä½•ç‰¹å®šçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
    - å¸¸è§çš„æŠ€æœ¯åŒ…æ‹¬ä½¿ç”¨ç»Ÿè®¡æŒ‡æ ‡ï¼ˆå¦‚ç›¸å…³ç³»æ•°ã€å¡æ–¹æ£€éªŒã€ä¿¡æ¯å¢ç›Šç­‰ï¼‰æ¥è¯„ä¼°ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
    - æ ¹æ®è¯„ä¼°ç»“æœï¼Œå¯ä»¥é€‰æ‹©æ’åæœ€é«˜çš„ç‰¹å¾ä½œä¸ºè¾“å…¥æ¨¡å‹çš„ç‰¹å¾ã€‚

- Wrapper methods â€“ itâ€™s a search for well-performing combinations of features
    - Wrapperæ–¹æ³•é€šè¿‡å°è¯•ä¸åŒçš„ç‰¹å¾å­é›†æ¥æœç´¢æœ€ä½³ç‰¹å¾ç»„åˆã€‚
    - å®ƒä¼šä½¿ç”¨ç‰¹å®šçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥è¯„ä¼°æ¯ä¸ªç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œå¹¶é€‰æ‹©æ€§èƒ½æœ€ä½³çš„ç‰¹å¾ç»„åˆä½œä¸ºæœ€ç»ˆé€‰æ‹©ã€‚
    - è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°ç‰¹å¾å­é›†çš„æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬é€šå¸¸æ¯”è¾ƒé«˜ï¼Œå› ä¸ºéœ€è¦å°è¯•è®¸å¤šå¯èƒ½çš„ç‰¹å¾ç»„åˆã€‚

- Embedded methods â€“ perform feature selection as part of the model training process.
    - Embeddedæ–¹æ³•å°†ç‰¹å¾é€‰æ‹©ä½œä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
    - è¿™æ„å‘³ç€ç‰¹å¾é€‰æ‹©ä¸æ¨¡å‹çš„è®­ç»ƒåŒæ—¶è¿›è¡Œï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨å­¦ä¹ å“ªäº›ç‰¹å¾å¯¹äºç»™å®šçš„ä»»åŠ¡æœ€é‡è¦ã€‚
    - ä¾‹å¦‚ï¼ŒæŸäº›æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆå¦‚å†³ç­–æ ‘ã€Lassoå›å½’ã€å²­å›å½’ç­‰ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨é€‰æ‹©å…·æœ‰è¾ƒé«˜æƒé‡çš„ç‰¹å¾ï¼Œè€Œå¿½ç•¥å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®è¾ƒå°çš„ç‰¹å¾ã€‚

## 2.7 Looking for Correlations ç›¸å…³æ€§

Correlation is a statistical analysis that is used to measure and describe the strength and direction of the relationship between two variables.

![](./Untitled10.png)

## 2.8 Feature Extraction ç‰¹å¾æå–

Technique in which new features are extracted from the existing ones.

- Identifying and selecting the most relevant and informative features from dataset
- Transforming them into a lower-dimensional space while preserving the most important information.

# LECTURE 3- DIMENSIONALITY REDUCTION

## 3.1 Why need Dimensionality Reduction

Data with high dimensions:
â€¢ High computational complexity è®¡ç®—å¤æ‚åº¦
â€¢ May contain many irrelevant or redundant features æœ‰å¯èƒ½åŒ…å«ä¸ç›¸å…³çš„ç‰¹å¾
â€¢ Difficulty in visualization ä¸å¥½å¯è§†åŒ–
â€¢ With high risk of getting an overfitting model å®¹æ˜“è¿‡æ‹Ÿåˆ

**Approaches for Dimensionality Reduction**

Projection: æŠ•å½±

- Data is not spread out uniformly across all dimensions. (All the data lies within (or close to) a much lower-dimensional subspace of the high-dimensional space

## 3.2 Principal Component Analysis (PCA)

PCA identifies the axis that accounts for the largest amount of variance in the training set

- PCAè¯†åˆ«è®­ç»ƒé›†ä¸­æ–¹å·®æœ€å¤§çš„è½´ â€” æ•°æ®ä¸­çš„ä¸»æˆåˆ†

å¤§è‡´æµç¨‹

1. **ä¸­å¿ƒåŒ–æ•°æ®**ï¼šé¦–å…ˆï¼Œå¯¹åŸå§‹æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†ï¼Œå³å°†æ¯ä¸ªç‰¹å¾çš„å‡å€¼å‡å»è¯¥ç‰¹å¾çš„å‡å€¼ï¼Œä½¿å¾—æ•°æ®çš„å‡å€¼ä¸º0ã€‚
2. **è®¡ç®—åæ–¹å·®çŸ©é˜µ**ï¼šç„¶åï¼Œè®¡ç®—ç‰¹å¾ä¹‹é—´çš„åæ–¹å·®çŸ©é˜µã€‚åæ–¹å·®çŸ©é˜µæè¿°äº†æ•°æ®ä¸­ä¸åŒç‰¹å¾ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚
3. **è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡**ï¼šæ¥ä¸‹æ¥ï¼Œé€šè¿‡å¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¾—åˆ°ç‰¹å¾å€¼å’Œå¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ç‰¹å¾å‘é‡ä»£è¡¨äº†æ•°æ®çš„ä¸»æˆåˆ†æ–¹å‘ï¼Œè€Œç‰¹å¾å€¼è¡¨ç¤ºäº†æ•°æ®åœ¨æ¯ä¸ªä¸»æˆåˆ†æ–¹å‘ä¸Šçš„æ–¹å·®å¤§å°ã€‚
4. **é€‰æ‹©ä¸»æˆåˆ†**ï¼šæ ¹æ®ç‰¹å¾å€¼çš„å¤§å°ï¼Œé€‰æ‹©å‰kä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ä½œä¸ºä¸»æˆåˆ†ã€‚é€šå¸¸ï¼Œå¯ä»¥æ ¹æ®ç‰¹å¾å€¼çš„å¤§å°æ¥ç¡®å®šä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ï¼Œä»¥ä¿ç•™è¶³å¤Ÿçš„æ•°æ®ä¿¡æ¯ã€‚
5. **æŠ•å½±æ•°æ®**ï¼šæœ€åï¼Œå°†åŸå§‹æ•°æ®æŠ•å½±åˆ°é€‰å®šçš„ä¸»æˆåˆ†ä¸Šï¼Œå¾—åˆ°é™ç»´åçš„æ•°æ®é›†ã€‚

![](./Untitled11.png)

C1 æŠ•å½±çš„æ•ˆæœæ›´å¥½

PCAçš„ç¼ºç‚¹åŒ…æ‹¬ï¼š

- æ— æ³•å¤„ç†éçº¿æ€§å…³ç³»çš„æ•°æ®ã€‚
- ä¸»æˆåˆ†é€šå¸¸æ˜¯åŸå§‹ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼Œå› æ­¤å¯èƒ½ä¸æ˜“è§£é‡Šã€‚
- åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè®¡ç®—åæ–¹å·®çŸ©é˜µå’Œç‰¹å¾å€¼åˆ†è§£çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚

## 3.3 Locally Linear Embedding (LLE)

LLE is a powerful nonlinear dimensionality reduction (NLDR) technique.
It is a Manifold Learning technique that does not rely on projections.

LLE (Locally Linear Embedding) æ˜¯ä¸€ç§å¼ºå¤§çš„éçº¿æ€§é™ç»´æŠ€æœ¯ï¼Œå®ƒå±äºæµå½¢å­¦ä¹ ï¼ˆManifold Learningï¼‰çš„èŒƒç•´ï¼Œä¸ä¼ ç»Ÿçš„æŠ•å½±æ–¹æ³•ä¸åŒï¼Œå®ƒé€šè¿‡å±€éƒ¨çº¿æ€§é‡æ„ä¿æŒäº†æ•°æ®çš„å±€éƒ¨ç»“æ„ã€‚

ä¸‹é¢æ˜¯ LLE çš„ä¸»è¦æ­¥éª¤ï¼š

1. **æ„å»ºé‚»åŸŸå›¾ï¼ˆNeighborhood Graphï¼‰**ï¼š
    - å¯¹äºç»™å®šçš„æ•°æ®é›†ï¼Œé¦–å…ˆç¡®å®šæ¯ä¸ªæ•°æ®ç‚¹çš„è¿‘é‚»ï¼Œå¯ä»¥ä½¿ç”¨æœ€è¿‘é‚»ç®—æ³•æˆ–è€…åŸºäºè·ç¦»çš„æ–¹æ³•ã€‚é€šå¸¸é‡‡ç”¨ K è¿‘é‚»ç®—æ³•ï¼Œé€‰æ‹©æ¯ä¸ªç‚¹çš„ K ä¸ªæœ€è¿‘é‚»ã€‚
    - åŸºäºè¿‘é‚»å…³ç³»æ„å»ºä¸€ä¸ªé‚»åŸŸå›¾ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®ç‚¹éƒ½ä¸å…¶è¿‘é‚»è¿æ¥ã€‚
2. **é‡æ„æƒé‡è®¡ç®—ï¼ˆCompute Reconstruction Weightsï¼‰**ï¼š
    - å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ $x_i$ï¼Œé€šè¿‡æœ€å°åŒ–é‡æ„è¯¯å·®æ¥è®¡ç®—å…¶å±€éƒ¨çº¿æ€§é‡æ„æƒé‡ã€‚è¿™å¯ä»¥é€šè¿‡å°† $x_i$é‡æ„ä¸ºå…¶è¿‘é‚»çš„çº¿æ€§ç»„åˆæ¥å®ç°ã€‚
    - å¯¹äºæ¯ä¸ª$x_i$ ï¼Œæ‰¾åˆ°æœ€ä½³çš„çº¿æ€§ç»„åˆç³»æ•° $w_{ij}$ï¼Œä½¿å¾— $x_i$ å¯ä»¥ç”±å…¶è¿‘é‚» $x_j$ é€šè¿‡çº¿æ€§ç»„åˆé‡æ„ã€‚
    - é€šå¸¸ä½¿ç”¨æœ€å°åŒ–é‡æ„è¯¯å·®çš„æ­£åˆ™åŒ–æ¡ä»¶ï¼Œå¦‚æœ€å°åŒ–æ¬§æ°è·ç¦»æˆ–è€…æœ€å°åŒ–è¯¯å·®çš„å¹³æ–¹å’Œã€‚
3. **åµŒå…¥ç©ºé—´å­¦ä¹ ï¼ˆLearn Embedding Spaceï¼‰**ï¼š
    - å°†æ•°æ®ç‚¹æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œä¿æŒå±€éƒ¨çº¿æ€§å…³ç³»ã€‚
    - å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ $x_i$ï¼Œä½¿ç”¨å…¶é‡æ„æƒé‡ $w_{ij}$ å¯¹å…¶é‚»åŸŸå†…çš„æ•°æ®ç‚¹è¿›è¡ŒåŠ æƒï¼Œç„¶åé€šè¿‡ç‰¹å¾åˆ†è§£ï¼ˆå¦‚å¥‡å¼‚å€¼åˆ†è§£ï¼‰è®¡ç®—åµŒå…¥ç©ºé—´çš„åæ ‡ã€‚
    - é€šå¸¸æƒ…å†µä¸‹ï¼Œé€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„ç›®æ ‡ç»´åº¦æ¥é™ä½æ•°æ®ç»´åº¦ï¼Œä¾‹å¦‚ 2D æˆ– 3Dã€‚
4. **å¯è§†åŒ–æˆ–ç‰¹å®šä»»åŠ¡åº”ç”¨ï¼ˆVisualization or Specific Task Applicationsï¼‰**ï¼š
    - åµŒå…¥ç©ºé—´çš„åæ ‡å¯ä»¥ç”¨äºå¯è§†åŒ–é«˜ç»´æ•°æ®é›†ï¼Œæˆ–è€…ä½œä¸ºç‰¹å®šä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ã€èšç±»ç­‰ï¼‰çš„è¾“å…¥ã€‚
    - å¯ä»¥ä½¿ç”¨å¸¸è§„çš„å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚ matplotlibï¼‰æ¥å¯è§†åŒ–é™ç»´åçš„æ•°æ®ã€‚

## 3.4 Other Dimensionality Reduction Techniques

![](./Untitled12.png)

# WEEK4  NaÃ¯ve Bayes æœ´ç´ è´å¶æ–¯

## 4.1 Bayesâ€™ Rule

Bayes' Rule is a fundamental concept in probability theory that relates the posterior probability of an event or hypothesis to the prior probability and the likelihood of observing evidence.

- æè¿°äº†åéªŒæ¦‚ç‡ä¸å…ˆéªŒæ¦‚ç‡ä»¥åŠè§‚å¯Ÿåˆ°çš„è¯æ®ä¹‹é—´çš„å…³ç³»

The formula for Bayes' Rule is: $P(c|x) = \frac{P(c) \times P(x|c)}{P(x)}$ 

![](./Untitled13.png)

![](./Untitled14.png)

**ä¸“æœ‰åè¯ï¼š**

**class-conditional probability (CCP) - ç±»æ¡ä»¶æ¦‚ç‡**

**prior probability - å…ˆéªŒæ¦‚ç‡**

**posterior probability - åéªŒæ¦‚ç‡**

**evidence factor - è¯æ®å› å­**

How can we make use of Bayesâ€™ Rule for Classification?

We want to maximise the posterior probability of observations

- æœ€å¤§åŒ–è§‚å¯Ÿçš„åéªŒæ¦‚ç‡
- This method is named MAP estimation (Maximum a posteriori)

## ä¾‹é¢˜

![](./Untitled15.png)

![](./Untitled16.png)

![](./Untitled17.png)

![](./Untitled18.png)

![](./Untitled19.png)

æ³¨æ„æ­£æ¯”äºï¼Œæ‰€ä»¥ä¸‹é¢çš„å°±ä¸ç”¨å»è®¡ç®—äº†

# **Lec5 Classification & Training models**

## MNIST æ•°æ®é›†

- 70,000 handwritten digits from 0-9
    - 60k Train, 10k Test Pre Split
- Each image has 784 features
    - 28 x 28 pixels
    - Each feature is pixel intensity from 0(white) to 255 (black)
- Benchmark for many models
- Balanced samples per class
- Not perfect - donâ€™t expect 100%

## **Binary Classification**

**Classificationï¼š**Classification algorithms find a function that determines which category the input data belongs to
**Binary Classificationï¼š**is a supervised learning algorithm that classifies new observations into one of two classes

## Performance Measures

Why do we need to evaluate machine learning models?

- The primary purpose of machine learning models is often to make a decision or develop insight. And in service of these goals, it is important to know **how much we can really trust that model and data.**
- Once you have built a machine learning framework (e,g. classifier), we should know it **performance** (e,g. accuracy).
- When you have a real-world problem, you would **compare different models** to pick the right one for it.

Metrics to evaluate **Classification** modelsï¼š

- Accuracy
- Confusion Matrix (not a metric but fundamental to others)
- Precision and Recall
- F1-score
- AUC&ROC

### Accuracy

Train/Test split:
We can split the entire dataset into train and test sets (e,g. 70% for training, 30% for testing). However, the generalization performance of a machine learning method relates to its prediction capability on independent test sets


![](./Untitled20.png)

### Cross Validation äº¤å‰éªŒè¯

Train/test/validation split 

To avoid selecting the parameters that perform best on the test data but maybe not the parameters that generalize best, we can further split the training set into training fold and validation fold


![](./Untitled21.png)

### **K-fold Cross-Validation kæŠ˜äº¤å‰æµ‹è¯•é›†è¿›è¡ŒéªŒè¯**


![](./Untitled22.png)

å°†æ•°æ®åˆ’åˆ†ä¸ºkä¸ªäº’æ–¥ä¸”å¤§å°ç›¸ç­‰çš„å­é›†ï¼Œ

è¿›è¡Œkæ¬¡è®­ç»ƒå’Œæµ‹è¯•ï¼Œæ¯æ¬¡é€‰å–**ç¬¬ i ä¸ªå­é›†**ä½œä¸ºæµ‹è¯•é›†ï¼ˆ1â‰¤iâ‰¤k)ï¼Œå…¶ä½™å­é›†ä½œä¸ºè®­ç»ƒé›†ã€‚ç”¨kæ¬¡æµ‹è¯•çš„ç»“æœçš„å¹³å‡æ•°å»è¯„ä¼°çš„æ¨¡å‹çš„performanceã€‚

è¯¥æ–¹æ³•å¯ä»¥é¿å…å›ºå®šåˆ’åˆ†æ•°æ®é›†çš„å±€é™æ€§ã€ç‰¹æ®Šæ€§ï¼Œè¿™ä¸ªä¼˜åŠ¿åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šæ›´æ˜æ˜¾ã€‚

- å°è§„æ¨¡æ•°æ®é›†çš„kåº”è¯¥åå¤§ï¼Œä»¥é‡‡çº³æ›´å¤§çš„è®­ç»ƒé›†ï¼ˆå°è®­ç»ƒé›†å¯èƒ½å¯¼è‡´è®­ç»ƒä¸å……åˆ†ï¼‰
- å¤§è§„æ¨¡æ•°æ®é›†çš„kåº”è¯¥åå°ï¼Œä»¥é‡‡çº³æ›´å¤§çš„æµ‹è¯•é›†ï¼ˆè®­ç»ƒé›†å¤Ÿå……åˆ†äº†ï¼‰

### Class imbalance æ ·æœ¬ç±»åˆ«ä¸å¹³è¡¡

### Confusion Matrix æ··æ·†çŸ©é˜µ

When performing classification or predictions, there are four types of outcomes that could occur:

- **True Positive (TP)**: Predict an observation belongs to a class and it actually does belong to that class.
- **True Negative (TN)**: Predict an observation does not belong to a class and it actually does not belong to that class.
- **False Positives (FP**): Predict an observation belongs to a class and it actually does not belong to that class.
- **False Negatives (FN)**: Predict an observation does not belong to a class and it actually does belong to that class


![](./Untitled23.png)

**Recall**ï¼ˆSensitivity): æ‰€æœ‰å®é™…ä¸ºæ­£çš„ä¸ªä½“ä¸­åˆ†ç±»ä¸ºæ­£çš„æ¯”ä¾‹

**Precisionï¼š**åœ¨æ‰€æœ‰è¢«åˆ†ç±»ä¸ºæ­£çš„ä¸ªä½“ä¸­å®é™…ä¸ºæ­£çš„æ¯”ä¾‹

### Trade off between precision and recall


![](./Untitled24.png)

- With precision - make sure what youâ€™re saying is positive is actually positive
- With recall - make sure youâ€™re not missing out on positive observations
- As one increases, the other decreases

- Metrics like **F1 scores** average them both

### **F1 scores**

Harmonic mean of precision and recall

- Gives more weight to low values
- Only get a high F1 score if **both are high**
- Typically precision & recall are similar


![](./Untitled25.png)

Depends on the situation

- Classifier to detect if videos are safe for kids
    - Reject many good videos (low recall) but keep safe one (high precision)
- Classifier to detect shoplifters?
    - May give false positives (high recall) but captures all thieves (low precision)
    

### Receiver Operation Characteristics (ROC)

A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. 

In another word, it presents **Recall (True Positive Rate) VS FPR ( False Positive Rate )**


![](./Untitled26.png)

The ROC graph summarizes all of the confusion matrices that each threshold produced

### Area Under the Curve(AUC)

AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0; one whose predictions are 100% correct has an AUC of 1.

![](./Untitled27.png)

## Multiclass Classification

Multiclass classification refers to classification tasks that can distinguish between **more than two classes.**

- One-versus-the-rest (OvR) strategy: train multiple binary classifiers for each class, select the class whose classifier outputs the highest score.
    - train N times
- One-versus-one (OvO) strategy: train a binary classifier for every pair of classes
    - train N(N-1)/2 times

Multilabel classification refers to classification system that outputs multiple binary tags

å¤šæ ‡ç­¾åˆ†ç±»æ˜¯æŒ‡**è¾“å‡ºå¤šä¸ªäºŒå…ƒæ ‡ç­¾çš„åˆ†ç±»ç³»ç»Ÿ**

## Regression

Regression attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).

- å›å½’è¯•å›¾ç¡®å®šä¸€ä¸ªå› å˜é‡ï¼ˆé€šå¸¸ç”¨ Y è¡¨ç¤ºï¼‰ä¸ä¸€ç³»åˆ—å…¶ä»–å˜é‡ï¼ˆç§°ä¸ºè‡ªå˜é‡ï¼‰ä¹‹é—´å…³ç³»çš„å¼ºåº¦å’Œç‰¹å¾ã€‚

![](./Untitled28.png)

### Simple Linear Regression

![](./Untitled29.png)

The difference between the fitted value ( predicted value) and real value is known as **residuals æ®‹å·®ã€‚**

![](./Untitled30.png)

### Linear Regression

**A linear model** makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term)

- çº¿æ€§æ¨¡å‹é€šè¿‡ç®€å•åœ°è®¡ç®—è¾“å…¥ç‰¹å¾çš„åŠ æƒå’ŒåŠ ä¸Šç§°ä¸ºåå·®é¡¹ï¼ˆä¹Ÿç§°ä¸ºæˆªè·é¡¹ï¼‰çš„å¸¸æ•°æ¥è¿›è¡Œé¢„æµ‹

![](./Untitled31.png)

**Cost function:** Mean Squared error (MSE) for a Linear Regression model

![](./Untitled32.png)

**Training the model is the process to find the value of $\theta$ that minimizes the cost function.**

### Gradient Descent

Minimize the Mean Squared error (MSE) cost function:

![](./Untitled33.png)

**Local minimum and Plateau å±€éƒ¨æå°å€¼å’Œé«˜åŸ**

![](./Untitled34.png)

The MSE cost function for a Linear Regression model is continuous and convex function.

- çº¿æ€§å›å½’æ¨¡å‹çš„ MSE æˆæœ¬å‡½æ•°æ˜¯è¿ç»­å‡½æ•°å’Œå‡¸å‡½æ•°ã€‚

Gradient Descent is guaranteed to approach arbitrarily close the global minimum.

- æ¢¯åº¦ä¸‹é™å¯ä»¥ä¿è¯ä»»æ„æ¥è¿‘å…¨å±€æœ€å°å€¼ã€‚

**ç§ç±»ï¼š**

- **Batch Gradient Descent (Full Gradient Descent ) æ‰¹é‡æ¢¯åº¦ä¸‹é™**
    - Use the whole training set to compute the gradients at every step.
    - åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†æ¥è®¡ç®—æ¢¯åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†æ‰€æœ‰è®­ç»ƒæ ·æœ¬è¾“å…¥æ¨¡å‹ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å…³äºæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦ï¼Œç„¶åæ ¹æ®è¿™ä¸ªæ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚ç”±äºéœ€è¦å¤„ç†æ•´ä¸ªè®­ç»ƒé›†ï¼Œå› æ­¤é€šå¸¸è®¡ç®—é‡è¾ƒå¤§ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§å‹æ•°æ®é›†ã€‚
- **Stochastic Gradient Descent (SGD) éšæœºæ¢¯åº¦ä¸‹é™**
    - Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance.
    - ä¸æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸åŒï¼Œéšæœºæ¢¯åº¦ä¸‹é™åœ¨æ¯ä¸€æ­¥ä¸­åªä½¿ç”¨ä¸€ä¸ªéšæœºæ ·æœ¬æ¥è®¡ç®—æ¢¯åº¦ã€‚å®ƒéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å…³äºè¯¥æ ·æœ¬çš„æ¢¯åº¦ï¼Œç„¶åæ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚ç”±äºæ¯æ¬¡åªä½¿ç”¨ä¸€ä¸ªæ ·æœ¬ï¼Œå› æ­¤è®¡ç®—é‡è¾ƒå°ï¼Œä½†æ›´æ–°å¯èƒ½æ›´åŠ ä¸ç¨³å®šï¼Œå› ä¸ºæ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦éƒ½å¯èƒ½ä¸åŒã€‚
- **Mini-batch Gradient Descent å°æ‰¹é‡æ¢¯åº¦ä¸‹é™**
    - Mini-batch GD computes the gradients on small random sets of instances called mini-batches

### Polynomial Regression

![](./Untitled35.png)

If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression. (Is high-degree polynomial always better?)

**Bias:** refers to the error from erroneous assumptions in the learning algorithm. (inability to capture the underlying patterns in the data).

åå·®ï¼šæ˜¯æŒ‡å­¦ä¹ ç®—æ³•ä¸­ç”±äºé”™è¯¯å‡è®¾è€Œäº§ç”Ÿçš„è¯¯å·®ã€‚ ï¼ˆæ— æ³•æ•è·æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼‰ã€‚

**Variance:** refers an error from sensitivity to small fluctuations in the training data. (difference in fits between data sets)

æ–¹å·®ï¼šæ˜¯æŒ‡å¯¹è®­ç»ƒæ•°æ®çš„å¾®å°æ³¢åŠ¨çš„æ•æ„Ÿæ€§æ‰€äº§ç”Ÿçš„è¯¯å·®ã€‚ ï¼ˆæ•°æ®é›†ä¹‹é—´çš„æ‹Ÿåˆå·®å¼‚ï¼‰

### Learning Curves å­¦ä¹ æ›²çº¿

![](./Untitled36.png)

## Regularized Linear Models

### **Ridge Regression(L2)**

- **è„Šå›å½’**æ˜¯ä¸€ç§çº¿æ€§å›å½’çš„æ‰©å±•ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼ˆL2èŒƒæ•°ï¼‰æ¥è§£å†³æ™®é€šæœ€å°äºŒä¹˜æ³•çš„ä¸€äº›é—®é¢˜ã€‚

Cost function:

![](./Untitled37.png)

This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. 

- åŠ ä¸Šä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œè¯¥æƒ©ç½šé¡¹æ­£æ¯”äºæ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œï¼Œå³L2èŒƒæ•°ã€‚è¿™ä¸ªæƒ©ç½šé¡¹æ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ï¼Œä½¿å¾—æ¨¡å‹æƒé‡å°½é‡å°ï¼Œä»è€Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

![](./Untitled38.png)

é€šè¿‡è°ƒèŠ‚ğ›¼çš„å€¼ï¼Œå¯ä»¥æ§åˆ¶æ¨¡å‹å¯¹äºæ‹Ÿåˆè®­ç»ƒæ•°æ®å’Œ**ä¿æŒæ¨¡å‹ç®€å•æ€§**ä¹‹é—´çš„å¹³è¡¡ã€‚

è¾ƒå¤§çš„*Î±*ä¼šå¯¼è‡´æ¨¡å‹æ›´è¶‹å‘äºç®€å•ï¼Œå‡å°‘è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€å®šçš„é¢„æµ‹å‡†ç¡®åº¦ã€‚

### **Lasso Regression(L1)**

- **å¥—ç´¢å›å½’**ä½¿ç”¨çš„æ˜¯**L1èŒƒæ•°ä½œä¸ºæ­£åˆ™åŒ–é¡¹**ï¼Œè€Œä¸æ˜¯L2èŒƒæ•°ã€‚

Cost function:

![](./Untitled39.png)

![](./Untitled40.png)

Lasso Regression automatically performs feature selection and outputs a sparse mode

- Lassoå›å½’çš„ä¸»è¦ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿè‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå¾—åˆ°ä¸€ä¸ªç¨€ç–æ¨¡å‹ï¼ˆå³ï¼Œæ¨¡å‹çš„æƒé‡ä¸­æœ‰å¾ˆå¤šä¸ºé›¶ï¼‰ã€‚
- ç”±äºå®ƒå€¾å‘äºä½¿å¾—ä¸€éƒ¨åˆ†ç‰¹å¾çš„æƒé‡å˜ä¸ºé›¶ï¼Œä»è€Œ**å®ç°äº†ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ**ã€‚
    - è¿™ä½¿å¾—å¥—ç´¢å›å½’åœ¨æŸäº›æƒ…å†µä¸‹æ›´æ˜“äºè§£é‡Šå’Œç†è§£ã€‚
- ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¥—ç´¢å›å½’çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¤§é‡ç‰¹å¾çš„æƒ…å†µä¸‹ã€‚

### **Elastic Net**

It is a middle ground between Ridge Regression and Lasso Regression.

- **å¼¹æ€§ç½‘ç»œ**åŒæ—¶ä½¿ç”¨L1èŒƒæ•°å’ŒL2èŒƒæ•°ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼Œç»“åˆäº†Lassoå›å½’çš„ç¨€ç–æ€§å’ŒRidgeå›å½’çš„æ­£åˆ™åŒ–ä¼˜åŠ¿ï¼Œå¯ä»¥åœ¨ç‰¹å¾æ•°é‡å¤§äºæ ·æœ¬æ•°é‡æˆ–ç‰¹å¾ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§è¾ƒé«˜æ—¶å–å¾—æ›´å¥½çš„æ•ˆæœã€‚

Cost function:

![](./Untitled41.png)

- å½“ğ‘Ÿ=0æ—¶ï¼Œå¼¹æ€§ç½‘ç»œé€€åŒ–ä¸ºè„Šå›å½’ï¼›
- å½“ğ‘Ÿ=1æ—¶ï¼Œå¼¹æ€§ç½‘ç»œé€€åŒ–ä¸ºå¥—ç´¢å›å½’ã€‚
- é€šè¿‡è°ƒèŠ‚ğ‘Ÿ

### **Early stopping æ—©åœ**

To stop training as soon as the validation error reaches a minimum.

![](./Untitled42.png)

## Logistic Regression

Logistic Regression is commonly used to **estimate the probability** that an instance belongs to a particular class.

- é€»è¾‘å›å½’æ˜¯ä¸€ç§å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç”¨äºè§£å†³äºŒåˆ†ç±»é—®é¢˜ï¼Œå³å°†å®ä¾‹åˆ†ä¸ºä¸¤ä¸ªç±»åˆ«ä¹‹ä¸€ã€‚
    - å®ƒé€šè¿‡é€»è¾‘å‡½æ•°ï¼ˆä¹Ÿç§°ä¸ºsigmoidå‡½æ•°ï¼‰æ¥ä¼°è®¡å®ä¾‹å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚
    - é€»è¾‘å›å½’çš„è¾“å‡ºæ˜¯ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ¦‚ç‡åˆ†æ•°ï¼Œå¯ä»¥ç”¨æ¥åšå‡ºäºŒå…ƒå†³ç­–

![](./Untitled43.png)

### Softmax Regression: for Multinomial Logistic Regression

![](./Untitled44.png)

# Week 6 Support Vector Machine

## Linear SVM

- SVM is a classifier derived from statistical learning theory by Vapnik and Chervonenkis in 1963.
- SVMs are learning systems thatï¼š
    - use a hyperplane of **linear functions**
        - ä½¿ç”¨çº¿æ€§å‡½æ•°çš„è¶…å¹³é¢
    - in a high dimensional feature space â€” **Kernel function**
        - åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­ â€“ æ ¸å‡½æ•°
    - trained with a learning algorithm from optimization theory â€” **Lagrangian duality**
        - æ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§
    - Implements a learning bias derived from statistical learning theory â€” **Generalization**
        - æ³›åŒ–

**Margin è¾¹è·** defined as the width of the decision boundary: 

- when the width of the decision boundary grows to exactly touch the instances.

### **Maximum margin linear classifier  æœ€å¤§è¾¹è·çº¿æ€§åˆ†ç±»å™¨**

A maximum margin linear classifier is a linear classifier with a maximum margin. It is the simplest form of Support Vector Machine (SVM), called Linear Support Vector Machine (Linear SVM, LSVM).

- æœ€å¤§é—´éš”çº¿æ€§åˆ†ç±»å™¨æ˜¯å…·æœ‰**æœ€å¤§è¾¹ç•Œçš„çº¿æ€§åˆ†ç±»å™¨**ã€‚
- æ˜¯æ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machineï¼ŒSVMï¼‰çš„ä¸€ç§æœ€ç®€å•å½¢å¼ï¼Œç§°ä¸ºçº¿æ€§æ”¯æŒå‘é‡æœºï¼ˆLinear SVMï¼ŒLSVMï¼‰ã€‚

The decision boundary could be much better if the feature is scaled

å¦‚æœå¯¹ç‰¹å¾è¿›è¡Œç¼©æ”¾ï¼Œå†³ç­–è¾¹ç•Œå¯èƒ½ä¼šæ›´å¥½

**ç¡¬é—´éš”æ”¯æŒå‘é‡æœºï¼ˆHard Margin SVMï¼‰**

- All instances being off the street and on the right side is named â€œhard margin classificationâ€
- The main limitation of hard margin classification is
    - The data must be linearly separable æ•°æ®å¿…é¡»æ˜¯çº¿æ€§å¯åˆ†çš„
    - Sensitive to outliers å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ

![](./Untitled45.png)

**è½¯é—´éš”æ”¯æŒå‘é‡æœºï¼ˆSoft Margin SVMï¼‰**

- æ˜¯æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰çš„ä¸€ç§å˜ä½“ï¼Œç”¨äºå¤„ç†çº¿æ€§ä¸å¯åˆ†çš„æƒ…å†µã€‚ä¸ç¡¬é—´éš”æ”¯æŒå‘é‡æœºï¼ˆHard Margin SVMï¼‰åªèƒ½å¤„ç†ä¸¥æ ¼çº¿æ€§å¯åˆ†çš„æ•°æ®é›†ä¸åŒï¼Œè½¯é—´éš”SVMå…è®¸åœ¨ä¸€å®šç¨‹åº¦ä¸Šå®¹å¿ä¸€äº›æ•°æ®ç‚¹çš„åˆ†ç±»é”™è¯¯ï¼Œä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

![](./Untitled46.png)

*C* æ˜¯æ­£åˆ™åŒ–å‚æ•°ï¼Œç”¨äºå¹³è¡¡é—´éš”çš„æœ€å¤§åŒ–å’Œè¯¯åˆ†ç±»çš„æƒ©ç½šã€‚

- A hyper-parameter C is defined
    - A low value of C leads to more margin violations
    - A high value of C limits the flexibility

![](./Untitled47.png)

### Non-Linear SVM Classification

- Classes are not linearly separable in the input space
- Project input space into a high dimensional feature space
- Polynomial Features
    - Polynomial features involve taking an existing feature and raising it to a power.
        - å¤šé¡¹å¼ç‰¹å¾æ¶‰åŠå°†ç°æœ‰ç‰¹å¾æå‡ä¸€ä¸ªå¹‚ã€‚
    - This is useful for capturing non-linear relationships between the feature and the target variable.
    - For example, if you have a feature X, polynomial features could include X^2, X^3, etc

é€šè¿‡å°†æ•°æ®æ˜ å°„åˆ°**é«˜ç»´ç©ºé—´**ï¼ŒåŸæœ¬çº¿æ€§ä¸å¯åˆ†çš„é—®é¢˜å¯ä»¥åœ¨è¿™ä¸ªç©ºé—´ä¸­å˜å¾—çº¿æ€§å¯åˆ†ã€‚ç„¶åï¼Œå¯ä»¥ä½¿ç”¨SVMåœ¨è¿™ä¸ªé«˜ç»´ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„çº¿æ€§è¶…å¹³é¢ï¼Œå°†ä¸åŒç±»åˆ«çš„æ ·æœ¬åˆ†å¼€ã€‚æœ€ç»ˆçš„åˆ†ç±»å†³ç­–æ˜¯åŸºäºåœ¨é«˜ç»´ç©ºé—´ä¸­æ‰¾åˆ°çš„è¶…å¹³é¢åœ¨åŸå§‹è¾“å…¥ç©ºé—´ä¸­çš„æŠ•å½±ã€‚

### Nonlinear SVM: Kernel Method

**Kernel Method:** 

- Ideologically, transform low-dimensional non-linear space to high-dimensional linear space, using kernel function.  ä»æ€æƒ³ä¸Šè®²ï¼Œä½¿ç”¨æ ¸å‡½æ•°å°†ä½ç»´éçº¿æ€§ç©ºé—´è½¬æ¢ä¸ºé«˜ç»´çº¿æ€§ç©ºé—´ã€‚

**Kernel Function:** 

- Kernel Function = $< âˆ… (x) , âˆ… (x^â€™) >$, $<.>$ means dot-product
- It covers non-linear transformations and an inner product operation on nonlinear transformations. å®ƒæ¶µç›–éçº¿æ€§å˜æ¢å’Œéçº¿æ€§å˜æ¢çš„å†…ç§¯è¿ç®—ã€‚

**Kernel Trick:** 

- Computationally, avoiding explicitly computing the transformation to another feature space.
    - Project input space into a very high-dimensional feature space, may be even infinity
    - Problem:
        - Projecting training data in to a high-dimensional space is expensive æˆæœ¬é«˜
        - large number of parameters å¤§é‡å‚æ•°
    - Trick:
        - Compute dot-product between training samples in the projected high-dimensional space without ever projecting. è®¡ç®—æŠ•å½±é«˜ç»´ç©ºé—´ä¸­è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„ç‚¹ç§¯ï¼Œæ— éœ€æŠ•å½±

![](./Untitled48.png)

### SVM Regression

SVM algorithm is versatile: Classification & Regression

Linear and nonlinear regression

![](./Untitled49.png)

# Week 7 Decision Trees and Random Forests

## Decision Tree

- A tree-like model that illustrates series of events leading to certain decisions
    - æ ‘çŠ¶æ¨¡å‹ï¼Œè¯´æ˜å¯¼è‡´æŸäº›å†³ç­–çš„ä¸€ç³»åˆ—äº‹ä»¶
- Each node represents a test on an attribute and each branch is an outcome of that test
    - æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨å¯¹å±æ€§çš„æµ‹è¯•ï¼Œæ¯ä¸ªåˆ†æ”¯éƒ½æ˜¯è¯¥æµ‹è¯•çš„ç»“æœ

**Depth**: the length of the longest path from the root node to a leaf node

Which attribute provides better separating? Whyï¼Ÿ

- Because the resulting subsets are more **pure**
- Knowing the value of this attribute gives us **more information** about the label
    - (the entropy of the subsets is low)

### Information Gain **ä¿¡æ¯å¢ç›Š**

**Entropy** measures the degree of randomness in data

![](./Untitled50.png)

- ***Lower entropy implies greater predictability!***
    - è¾ƒä½çš„ç†µæ„å‘³ç€æ›´é«˜çš„å¯é¢„æµ‹æ€§ï¼
- For a set of samples $X$ with $k$ classes:
    
![](./Untitled51.png)
    
    - where $p_i$  is the proportion of elements of class $i$

The information gain of an attribute a is the expected reduction in entropy due to splitting on values of a.

- å±æ€§ a çš„ä¿¡æ¯å¢ç›Šæ˜¯ç”±äºå¯¹ a çš„å€¼è¿›è¡Œåˆ†å‰²è€Œå¯¼è‡´çš„é¢„æœŸç†µå‡å°‘

![](./Untitled52.png)

where $X_v$  is the subset of $X$  for which $a = v$

**Best attribute = highest information gain**

### Gini Impurity åŸºå°¼æ‚è´¨

- Gini impurity measures how often a randomly chosen example would be incorrectly labeled if it was randomly labeled according to the label distribution
    - è¡¨ç¤ºä¸€ä¸ªæ•°æ®é›†çš„ä¸çº¯åº¦æˆ–æ··ä¹±ç¨‹åº¦ã€‚
    - Giniä¸çº¯åº¦çš„å€¼ä»‹äº**0åˆ°0.5**ä¹‹é—´:
        - å…¶ä¸­0è¡¨ç¤ºæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾å®Œå…¨ç›¸åŒï¼ˆçº¯åº¦æœ€é«˜ï¼‰
        - è€Œ0.5è¡¨ç¤ºæ•°æ®é›†ä¸­çš„æ ‡ç­¾å‡åŒ€åˆ†å¸ƒï¼ˆä¸çº¯åº¦æœ€é«˜ï¼‰
- ***Can be used as an alternative to entropy for selecting attributes!***
    - å¯æ›¿ä»£ç†µä½œä¸ºå±æ€§çš„è¯„åˆ¤æ ‡å‡†

**Best attribute = lowest Gini impurity**

![](./Untitled53.png)


### CART Algorithmï¼ˆåˆ†ç±»å’Œå›å½’æ ‘ï¼‰

**Classification and Regression Tree  ï¼ˆåˆ†ç±»å’Œå›å½’æ ‘ï¼‰**

Splits the training set into two subsets using a single feature ($k$) and a threshold ($t_k$)

- **å•ä¸ªç‰¹å¾** ($*k*$) å’Œ é˜ˆå€¼ ($t_k$)ï¼šé€šè¿‡é€‰æ‹©ä¸€ä¸ªç‰¹å¾ $*k$*  å’Œç›¸åº”çš„é˜ˆå€¼ $t_k$ï¼Œå°†æ•°æ®é›†åˆ‡åˆ†æˆä¸¤ä¸ªå­é›†ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°èƒ½å¤Ÿæœ€å°åŒ–ä»£ä»·å‡½æ•°ï¼ˆä¸çº¯åº¦æˆ–å‡æ–¹è¯¯å·®ï¼‰çš„æœ€ä½³ç‰¹å¾å’Œé˜ˆå€¼ç»„åˆã€‚

![](./Untitled54.png)

### Regularization

![](./Untitled55.png)

- Decision trees produce non-linear decision boundaries
    - å†³ç­–æ ‘äº§ç”Ÿéçº¿æ€§å†³ç­–è¾¹ç•Œ

## Ensemble Learning

**Ensemble : A group of predictors**

![](./Untitled56.png)
### Bagging and Pasting/Random Subspace

Training with different subsets of data

Bagging: Sampling with Replacement

- ä»è®­ç»ƒé›†ä¸­æœ‰æ”¾å›åœ°æŠ½æ ·ï¼Œé€‚åˆå‡å°‘æ¨¡å‹æ–¹å·®ã€‚

Pasting: Sampling without Replacement

- ä»è®­ç»ƒé›†ä¸­ä¸æ”¾å›åœ°æŠ½æ ·ï¼Œé€‚åˆä¸éœ€è¦é‡å¤æ ·æœ¬çš„å¤§è®­ç»ƒé›†ã€‚

**Random Subspace**

- éšæœºå­ç©ºé—´æ–¹æ³•æ˜¯é’ˆå¯¹ç‰¹å¾è¿›è¡Œé‡‡æ ·ï¼Œè€Œä¸æ˜¯æ ·æœ¬ã€‚
- éšæœºé€‰æ‹©éƒ¨åˆ†**ç‰¹å¾**ï¼Œ**é€‚åˆé«˜ç»´æ•°æ®é›†**ï¼Œå¢åŠ æ¨¡å‹å¤šæ ·æ€§ã€‚
- **åŸç†** : åœ¨æ¯æ¬¡è®­ç»ƒæ¨¡å‹æ—¶ï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾è€Œä¸æ˜¯å…¨éƒ¨ç‰¹å¾ã€‚
- **æ­¥éª¤** :
    1. éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾ã€‚
    2. ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚
    3. é›†æˆè¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚
- **ä¼˜ç‚¹** : é€šè¿‡é™ä½ç‰¹å¾ç›¸å…³æ€§å’Œå¢åŠ æ¨¡å‹å¤šæ ·æ€§æ¥æé«˜é›†æˆæ¨¡å‹çš„æ€§èƒ½ã€‚

## Random Forests

Random Forests are one of the most common examples of ensemble learning

### Boosting

Boosting: train models iteratively, while making the current model focus on the mistakes of the previous ones by increasing the weight of misclassified samples

- Boostingé€šè¿‡è¿­ä»£è®­ç»ƒå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼Œæ¯ä¸ªå­¦ä¹ å™¨éƒ½ä¸“æ³¨äºå‰ä¸€ä¸ªå­¦ä¹ å™¨çš„é”™è¯¯åˆ†ç±»æ ·æœ¬ï¼Œä»è€Œé€æ­¥æé«˜æ•´ä½“æ¨¡å‹çš„æ€§èƒ½ã€‚
- å¸¸è§çš„Boostingç®—æ³•åŒ…æ‹¬AdaBoostå’ŒGradient Boostingï¼Œå®ƒä»¬åˆ†åˆ«é€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡å’Œæ‹Ÿåˆæ®‹å·®æ¥ä¼˜åŒ–æ¨¡å‹ã€‚

### Stacking

![](./Untitled57.png)
- Stackingé€šè¿‡**ç»„åˆå¤šä¸ªä¸åŒç±»å‹çš„æ¨¡å‹**æ¥æé«˜æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚
- å®ƒçš„åŸºæœ¬æ€è·¯æ˜¯ä½¿ç”¨åŸºå­¦ä¹ å™¨ç”Ÿæˆä¸€çº§é¢„æµ‹ç»“æœï¼Œç„¶åç”¨æ¬¡çº§å­¦ä¹ å™¨å°†è¿™äº›é¢„æµ‹ç»“æœç»„åˆèµ·æ¥
- å°½ç®¡Stackingçš„è®¡ç®—å¤æ‚æ€§è¾ƒé«˜ï¼Œä½†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œé€šå¸¸èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚

# Week 9 èšç±»ç®—æ³•

## K-Means

![](./Untitled58.png)
**Supervised learning**

- The correct labels for each training example are known

**Unsupervised learning**

- Labels are unknown
- Need to **automatically discover the clustering pattern** and structure in data

**K-means clustering algorithm**

Goal: Assign all data points to 2ï¼ˆnï¼‰ clusters

1. **é€‰æ‹©åˆå§‹è´¨å¿ƒ**:
    - éšæœºé€‰æ‹©ä¸¤ä¸ªæ•°æ®ç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒã€‚
    - ä¾‹å¦‚ï¼Œ(x1, y1) ä½œä¸ºçº¢è‰²è´¨å¿ƒï¼Œ(x2, y2) ä½œä¸ºè“è‰²è´¨å¿ƒã€‚
2. **åˆ†é…æ•°æ®ç‚¹**:
    - è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹åˆ°çº¢è‰²å’Œè“è‰²è´¨å¿ƒçš„æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚
    - æ ¹æ®è·ç¦»çš„å¤§å°ï¼Œå°†æ•°æ®ç‚¹æ ‡è®°ä¸ºçº¢è‰²æˆ–è“è‰²ã€‚
3. **é‡æ–°è®¡ç®—è´¨å¿ƒ**:
    - å¯¹äºçº¢è‰²ç°‡ï¼Œè®¡ç®—æ‰€æœ‰çº¢è‰²æ•°æ®ç‚¹çš„å¹³å‡å€¼ï¼Œæ›´æ–°çº¢è‰²è´¨å¿ƒä½ç½®ã€‚
    - å¯¹äºè“è‰²ç°‡ï¼Œè®¡ç®—æ‰€æœ‰è“è‰²æ•°æ®ç‚¹çš„å¹³å‡å€¼ï¼Œæ›´æ–°è“è‰²è´¨å¿ƒä½ç½®ã€‚
4. **è¿­ä»£**:
    - é‡å¤æ­¥éª¤2å’Œæ­¥éª¤3ï¼Œç›´åˆ°è´¨å¿ƒä½ç½®ä¸å†å˜åŒ–ã€‚

![](./Untitled59.png)
æ³¨æ„ city block distance è®¡ç®—æ–¹æ³•

### Silhouette Coefficient è½®å»“ç³»æ•°

**silhouette coefficient** Measures the tightness of clusters and separation between clustersï¼š

![](./Untitled60.png)

where:

- $a(x_i)$  is the average distance between $x_i$  and all other points in the same cluster
- $b(x_i)$ is the average distance between $x_i$  and all other points in the next neighbor cluster
(i.e., the average distance to the nearest neighboring cluster)

- è½®å»“ç³»æ•°é€šè¿‡ç»¼åˆè€ƒè™‘ç‚¹åˆ°å…¶ç°‡å†…å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»ï¼ˆå‡èšåº¦ï¼‰å’Œç‚¹åˆ°æœ€è¿‘ä¸åŒç°‡çš„å¹³å‡è·ç¦»ï¼ˆåˆ†ç¦»åº¦ï¼‰ï¼Œæä¾›äº†ä¸€ç§**è¡¡é‡èšç±»æ•ˆæœçš„æœ‰æ•ˆæ–¹æ³•**ã€‚
- **å€¼è¶Šå¤§è¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½**
    - è½®å»“ç³»æ•°çš„å€¼åœ¨-1åˆ°1ä¹‹é—´ï¼Œå…¶ä¸­1è¡¨ç¤ºèšç±»æ•ˆæœéå¸¸å¥½ï¼Œ0è¡¨ç¤ºèšç±»è´¨é‡è¾ƒå·®ï¼Œè€Œè´Ÿå€¼è¡¨ç¤ºå¯èƒ½å­˜åœ¨é”™è¯¯çš„èšç±»åˆ†é…ã€‚
- é€šè¿‡è®¡ç®—è½®å»“ç³»æ•°ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯„ä¼°å’Œæ”¹è¿›èšç±»ç®—æ³•çš„æ€§èƒ½ã€‚

![](./Untitled61.png)

### K-means: pros and cons

- Pros:
    - Easy to implement
    - Scales to very large datasets
- Cons:
    - Difficult to choose K.
    - Only works on spherical, convex clusters

## Hierarchical Clustering

Hierarchical Clustering is a set of clustering methods that aim at building a hierarchy of clusters

- A cluster is composed of smaller clusters

There are two strategies for building the hierarchy of clusters:

- Agglomerative (bottom-up): we start with each point in its own cluster and we merge pairs of clusters until only one cluster is formed.
- Divisive (top-down): we start with a single cluster containing the entire set of points and we recursively split until each point is in its own cluster.
    - The most popular strategy in practical use is bottom-up (agglomerative)

### Hierarchical clustering- Agglomerative

**Idea:** make sure nearby data points end up in the same cluster

![](./Untitled62.png)

Distance options:

- **single linkage (closest pair)**:
the minimum distance between samples in sub-clusters
- **complete linkage (farthest pair)ï¼š**
maximum distance between samples in sub-clusters
- **average linkage (average of all pairs) :**
average distance between each pair of samples in subclusters

![](./Untitled63.png)

### Hierarchical clustering: pros and cons

- Pros
    - Hierarchical structure is more informative than flat clusters (K-means)
    - Easier to decide the number of clusters
- Cons:
    - Slow to compute:
        - Time complexity $O(n^3)$.
    - Sensitive to outliers, because it tries to connect all data points.

## DBSCAN

DBSCAN  - (Density-based spatial clustering of applications with noise )

**Density-based clustering**

**Idea:**  Clustering based on density (local clustering criterion),

- e.g. number of densely connected points

![](./Untitled64.png)

### DBSCAN pros and cons

- Pros:
    - Pretty fast:
        - Time complexity is $O(nlogn)$ when optimized.
    - Can find arbitrarily shaped clusters
    - Robust to outliers (recognized as noise points)
- Cons:
    - Cannot work well if density varies in different regions of data
    - Choosing a proper distance threshold Îµ can be difficult

# Week 10 Gaussian mixture model (GMM)

## Mixture Gaussian Model and EM method

Motivation:
K-means make hard assignments to data points: 

- $x^{(i)}$ must belong to one of the clusters $1,2, â‹¯ ,K$
- Sometimes, **one data point can belong to multiple clusters**

So:

- Clusters may overlap
- Hard assignment may be simplistic
- Need a soft assignment:
    - data points belong to clusters with different **probabilities**

## Gaussian (Normal) distribution

![](./Untitled65.png)

## Maximum Likelihood Estimation (MLE)

![](./Untitled66.png)

# KNN

### **k-NNé‚»è¿‘ç®—æ³•(k-nearest neighbors algorithm k-NN)**

å°†æµ‹è¯•é›†æ•°æ®xæ”¾åœ¨è®­ç»ƒé›†çš„ç‰¹å¾ç©ºé—´å†…ï¼Œå°†å…¶æƒ³è±¡æˆä¸€ä¸ªä¸æ–­å˜å¤§çš„çƒï¼Œéšç€çƒçš„å˜å¤§ï¼Œä»–ä¼šæŒ¨ä¸ªæ¥è§¦knä¸ªç¦»è‡ªå·±è¿‘çš„è®­ç»ƒé›†ï¼Œå°†è¿™knä¸ªè®­ç»ƒé›†æ ·æœ¬ç§°ä¸ºxçš„knè¿‘é‚»æ ·æœ¬ï¼ˆknÂ nearest-neighbours of xï¼‰ï¼Œçœ‹çœ‹è¿™kä¸ªè¿‘é‚»æ ·æœ¬ä¸­å“ªä¸ªlabelå æ¯”æœ€å¤§ï¼Œxå°±å±äºé‚£ä¸ªlabelã€‚

kæ˜¯è‡ªå·±è®¾ç«‹çš„å€¼ï¼Œæ¯ä¸ªxæ‹¥æœ‰kä¸ªè¿‘é‚»æ ·æœ¬ï¼Œå­¦ä¹ çš„è¿‡ç¨‹å°±æ˜¯é€‰æ‹©ä¸€ä¸ªèƒ½ä½¿å¾—æ¨¡å‹æ•ˆæœæœ€å¥½çš„kçš„å€¼çš„è¿‡ç¨‹ã€‚ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ç›´æ¥ç¡®å®škå€¼ï¼Œå­¦æ ¡PPTç”¨çš„æ˜¯kn=âˆšnï¼‰

ä¸¤ç§æƒ…å†µï¼š

1. xç‚¹å‘¨å›´çš„è®­ç»ƒé›†æ ·æœ¬å¯†é›†ï¼Œxçš„è¿‘é‚»æ ·æœ¬ç¦»è‡ªå·±éå¸¸è¿‘ã€‚
2. xç‚¹å‘¨å›´çš„è®­ç»ƒé›†æ ·æœ¬ç¨€ç–ï¼Œxéœ€è¦æŒç»­æ‰¾åˆ°kä¸ªè·ç¦»è‡ªå·±ç¨è¿œçš„è¿‘é‚»æ ·æœ¬ã€‚
